{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4709aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "#  Sequential,\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Conv2D, AveragePooling2D, Embedding, LSTM, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#  GRU, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# from transformers import BertTokenizerFast, TFBertModel\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "\n",
    "from scripts.word_embeddings import load_embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fedca1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(sentences_tokens):\n",
    "    voc = set()\n",
    "    \n",
    "    voc.update(['<PAD>'])\n",
    "    \n",
    "    for tokens in sentences_tokens:\n",
    "        voc.update(tokens)\n",
    "        \n",
    "    voc = list(voc)\n",
    "        \n",
    "    word_to_id = {word: index for word, index in zip(voc, range(len(voc)))}\n",
    "    id_to_word = {index: word for word, index in zip(voc, range(len(voc)))}\n",
    "    \n",
    "#     print(word_to_id['<PAD>'])\n",
    "    \n",
    "    return voc, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1204f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(word):\n",
    "    return re.sub(r'[\\-\\(\\)\\?\\*\\\\\\ \\'\\\"\\.%\\^,:<>|;]', '', word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bff9bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/petar/Fakultet/Semester 7/NLP/Datasets/fake_news/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55802dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(path + 'full_train_df.csv', index_col=0)\n",
    "test = pd.read_csv(path + 'full_test_df.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93255c12",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc51f09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet_tokens'] = train['tweet'].apply(lambda sentence: sentence.split(' '))\n",
    "test['tweet_tokens'] = test['tweet'].apply(lambda sentence: sentence.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aee1251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet_tokens'] = train['tweet_tokens'].apply(lambda sentence: [clean(word) for word in sentence])\n",
    "test['tweet_tokens'] = test['tweet_tokens'].apply(lambda sentence: [clean(word) for word in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02cba80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18501/1723939186.py:1: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tmp = train['tweet_tokens'].append(test['tweet_tokens'])\n"
     ]
    }
   ],
   "source": [
    "tmp = train['tweet_tokens'].append(test['tweet_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4915e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.index = range(len(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "219686d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc, word_to_id, id_to_word = create_vocabulary(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfbef8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_embedding_weights(voc, 50, 'glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08836dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet_indices'] = train['tweet_tokens'].apply(lambda sentence_tokens: [word_to_id[token] for token in sentence_tokens])\n",
    "test['tweet_indices'] = test['tweet_tokens'].apply(lambda sentence_tokens: [word_to_id[token] for token in sentence_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "decac230",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.groupby(['user', 'label'])['tweet_indices'].agg(list).reset_index()\n",
    "test = test.groupby(['user', 'label'])['tweet_indices'].agg(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f713861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['tweet_indices'] = [pad_sequences(x, value=word_to_id['<PAD>'], maxlen=100) for x in train['tweet_indices']]\n",
    "# test['tweet_indices'] = [pad_sequences(x, value=word_to_id['<PAD>'], maxlen=100) for x in test['tweet_indices']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e170854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_indices = train['tweet_indices'].copy()\n",
    "y = train['label'].copy()\n",
    "# y = y.apply(lambda x: [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32d17413",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_x = [pad_sequences(x, value=word_to_id['<PAD>'], maxlen=100) for x in tweet_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "39e231c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_x = pd.Series(padded_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8eecb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(padded_x, y, test_size=.1, random_state=0, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39452c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = shuffle(padded_x, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d848c378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18501/4134551260.py:1: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  x_train = x_train.append(x_test)\n",
      "/tmp/ipykernel_18501/4134551260.py:2: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  y_train = y_train.append(y_test)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.append(x_test)\n",
    "y_train = y_train.append(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c395b41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_indices = test['tweet_indices'].copy()\n",
    "y = test['label'].copy()\n",
    "# y = y.apply(lambda x: [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "134cd235",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_x = [pad_sequences(x, value=word_to_id['<PAD>'], maxlen=100) for x in tweet_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73d70590",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_x = pd.Series(padded_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dfa06a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(padded_x, y, test_size=.1, random_state=0, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c16d46ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18501/1631974232.py:1: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  x_test = x_train.append(x_test)\n",
      "/tmp/ipykernel_18501/1631974232.py:2: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  y_test = y_train.append(y_test)\n"
     ]
    }
   ],
   "source": [
    "x_test = x_train.append(x_test)\n",
    "y_test = y_train.append(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8aad5594",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embeddings.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "32e5de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tmp = x_train.to_list()\n",
    "y_train_tmp = y_train.to_list()\n",
    "\n",
    "x_test_tmp = x_test.to_list()\n",
    "y_test_tmp = y_test.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2487ef",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "812b18d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-24 15:35:05.283902: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-06-24 15:35:05.284015: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (petar-X580VD): /proc/driver/nvidia/version does not exist\n",
      "2022-06-24 15:35:05.286182: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"conv2d\" is incompatible with the layer: expected min_ndim=4, found ndim=3. Full shape received: (None, None, 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model1 \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[1;32m      3\u001b[0m model1\u001b[38;5;241m.\u001b[39madd(Embedding(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(embeddings), output_dim\u001b[38;5;241m=\u001b[39membeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], weights\u001b[38;5;241m=\u001b[39m[embeddings], trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLeakyReLU\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m model1\u001b[38;5;241m.\u001b[39madd(GlobalAveragePooling2D())\n\u001b[1;32m     10\u001b[0m model1\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/home/petar/jupyter/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:587\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 587\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    589\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/home/petar/jupyter/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/home/petar/jupyter/lib/python3.8/site-packages/keras/engine/input_spec.py:228\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    226\u001b[0m   ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n\u001b[1;32m    227\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m<\u001b[39m spec\u001b[38;5;241m.\u001b[39mmin_ndim:\n\u001b[0;32m--> 228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    229\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    230\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpected min_ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mmin_ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    231\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfound ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    232\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# Check dtype.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"conv2d\" is incompatible with the layer: expected min_ndim=4, found ndim=3. Full shape received: (None, None, 50)"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Embedding(input_dim=len(embeddings), output_dim=embeddings.shape[1], weights=[embeddings], trainable=True))\n",
    "\n",
    "model1.add(Conv2D(128, 32, padding='valid', activation=tf.keras.layers.LeakyReLU(alpha=.1)))\n",
    "model1.add(GlobalAveragePooling2D())\n",
    "\n",
    "\n",
    "\n",
    "model1.add(Dense(64, activation='relu'))\n",
    "model1.add(Dense(32, activation='relu'))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model1.compile(optimizer=Adam(learning_rate=.00001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a0a316ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'numpy.ndarray'>\"}), <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [190]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test_tmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_tmp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/petar/jupyter/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/home/petar/jupyter/lib/python3.8/site-packages/keras/engine/data_adapter.py:985\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    982\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ALL_ADAPTER_CLS \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcan_handle(x, y)]\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapter_cls:\n\u001b[1;32m    984\u001b[0m   \u001b[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[0;32m--> 985\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    986\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to find data adapter that can handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    988\u001b[0m           _type_name(x), _type_name(y)))\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(adapter_cls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    990\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    991\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData adapters should be mutually exclusive for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandling inputs. Found multiple adapters \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    993\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    994\u001b[0m           adapter_cls, _type_name(x), _type_name(y)))\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'numpy.ndarray'>\"}), <class 'int'>"
     ]
    }
   ],
   "source": [
    "model1.fit(x_test_tmp, y_test_tmp[0], epochs=20, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a26fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
